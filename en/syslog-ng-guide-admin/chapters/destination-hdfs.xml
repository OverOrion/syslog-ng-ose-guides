<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section
[
<!ENTITY % entities SYSTEM "../../shared/syslog-ng-entities.ent">
%entities;
]>
<section xml:id="configuring-destinations-hdfs" xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title><parameter>hdfs</parameter>: Storing messages on the Hadoop Distributed File System (HDFS)</title>
    <indexterm>
        <primary>destination drivers</primary>
        <secondary><parameter>java()</parameter> driver</secondary>
    </indexterm>
    <indexterm>
        <primary>destination drivers</primary>
        <secondary><parameter>hdfs</parameter></secondary>
    </indexterm>
    <para>Starting with version <phrase condition="pe">5.3</phrase><phrase condition="ose">3.7</phrase>, &abbrev; can send plain-text log files to the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://hadoop.apache.org/">Hadoop Distributed File System (HDFS)</link>, allowing you to store your log data on a distributed, scalable file system. This is especially useful if you have huge amounts of log messages that would be difficult to store otherwise, or if you want to process your messages using Hadoop tools (for example, Apache Pig).</para>
    <para>For more information about the benefits of using syslog-ng as a data collection, processing, and filtering tool in a Hadoop environment, see the blog post <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://syslog-ng.com/blog/filling-your-data-lake-with-log-messages-the-syslog-ng-hadoop-hdfs-destination/">Filling your data lake with log messages: the syslog-ng Hadoop (HDFS) destination</link>.</para>
    <para>Note the following limitations when using the &abbrev; <parameter>hdfs</parameter> destination:</para>
    <itemizedlist>
        <xi:include href="../../shared/chunk/listitem-java-supported-platforms.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        <listitem>
            <para>Since &abbrev; uses the official Java HDFS client, the <parameter>hdfs</parameter> destination has significant memory usage (about 400MB).</para>
        </listitem>
        <listitem>
            <xi:include href="../../shared/chunk/para-hdfs-flush.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </listitem>
        <xi:include href="../../shared/chunk/listitem-java-internal-messages.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </itemizedlist>
    <formalpara>
        <title>Declaration:</title>
        <para/>
    </formalpara>
    <synopsis>@module mod-java
@include "scl.conf"

hdfs(
    client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:&lt;path-to-preinstalled-hadoop-libraries&gt;")
    hdfs-uri("hdfs://NameNode:8020")
    hdfs-file("&lt;path-to-logfile&gt;")
);</synopsis>
    <example xml:id="example-destination-hdfs">
        <title>Storing logfiles on HDFS</title>
        <para>The following example defines an <parameter>hdfs</parameter> destination using only the required parameters.</para>
        <synopsis>@module mod-java
@include "scl.conf"

destination d_hdfs {
    hdfs(
        client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:/opt/hadoop/libs")
        hdfs-uri("hdfs://10.140.32.80:8020")
        hdfs-file("/user/log/logfile.txt")
    );
};
</synopsis>
    </example>
    <itemizedlist>
        <listitem>
            <para>To install the software required for the <parameter>hdfs</parameter> destination, see <xref linkend="destination-hdfs-prerequisites"/>.</para>
        </listitem>
        <listitem>
            <para>For details on how the <parameter>hdfs</parameter> destination works, see <xref linkend="destination-hdfs-interaction"/>.</para>
        </listitem>
        <listitem>
            <para>For details on using MapR-FS, see <xref linkend="destination-hdfs-maprfs"/>.</para>
        </listitem>
        <listitem>
            <para>For details on using Kerberos authentication, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
        </listitem>
        <listitem>
            <para>For the list of options, see <xref linkend="reference-destination-hdfs"/>.</para>
        </listitem>
    </itemizedlist>
    <para condition="ose">The <parameter>hdfs()</parameter> driver is actually a reusable configuration snippet configured to receive log messages using the Java language-binding of &abbrev;. For details on using or writing such configuration snippets, see <xref linkend="config-blocks"/>. You can find the source of the hdfs configuration snippet on <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="https://github.com/balabit/syslog-ng/blob/master/scl/hdfs/plugin.conf">GitHub</link>. For details on extending &abbrev; in Java, see the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="https://syslog-ng.gitbooks.io/getting-started/content/chapters/chapter_5/section_2.html">Getting started with syslog-ng development</link> guide.</para>
    <xi:include href="destination-hdfs-prerequisites.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    <xi:include href="destination-hdfs-interaction.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    <xi:include href="destination-hdfs-maprfs.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    <section xml:id="destination-hdfs-kerberos-authentication">
        <title>Kerberos authentication with syslog-ng hdfs() destination</title>
        <para>Version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later supports Kerberos authentication to authenticate the connection to your Hadoop cluster. &abbrev; assumes that you already have a Hadoop and Kerberos infrastructure.</para>
        <note>
            <para>If you configure Kerberos authentication for a <parameter>hdfs()</parameter> destination, it affects all <parameter>hdfs()</parameter> destinations. Kerberos and non-Kerberos <parameter>hdfs()</parameter> destinations cannot be mixed in a &abbrev; configuration. This means that if one <parameter>hdfs()</parameter> destination uses Kerberos authentication, you have to configure all other <parameter>hdfs()</parameter> destinations to use Kerberos authentication too.</para>
            <para>Failing to do so results in non-Kerberos <parameter>hdfs()</parameter> destinations being unable to authenticate to the HDFS server.</para>
        </note>
        <note>
            <para>If you want to configure your <parameter>hdfs()</parameter> destination to stop using Kerberos authentication, namely, to remove Kerberos-related options from the <parameter>hdfs()</parameter> destination configuration, make sure to restart &abbrev; for the changes to take effect.</para>
        </note>
        <formalpara>
            <title>Prerequisites:</title>
            <para/>
        </formalpara>
        <itemizedlist>
            <listitem>
                <para>You have configured your Hadoop infrastructure to use Kerberos authentication.</para>
            </listitem>
            <listitem>
                <para>You have a keytab file and a principal for the host running &abbrev;. For details, see the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://web.mit.edu/Kerberos/krb5-1.5/krb5-1.5.4/doc/krb5-install/The-Keytab-File.html">Kerberos documentation</link>.</para>
            </listitem>
            <listitem>
                <para>You have installed and configured the Kerberos client packages on the host running &abbrev;. (That is, Kerberos authentication works for the host, for example, from the command line using the <command>kinit user@REALM -k -t &lt;keytab_file&gt;</command> command.)</para>
            </listitem>
        </itemizedlist>
        <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </section>
    <section xml:id="reference-destination-hdfs">
        <title>HDFS destination options</title>
        <indexterm>
            <primary>destination drivers</primary>
            <secondary><parameter>java()</parameter> driver</secondary>
        </indexterm>
        <indexterm>
            <primary>destination drivers</primary>
            <secondary><parameter>hdfs</parameter></secondary>
        </indexterm>
        <para>The <parameter>hdfs</parameter> destination stores the log messages in files on the Hadoop Distributed File System (HDFS). The <parameter>hdfs</parameter> destination has the following options.</para>
        <para>The following options are required: <parameter>hdfs-file()</parameter>, <parameter>hdfs-uri()</parameter>. Note that to use <parameter>hdfs</parameter>, you must add the following lines to the beginning of your &abbrev; configuration:</para>
        <synopsis>@module mod-java
@include "scl.conf"</synopsis>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-java-class-path.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>For the <parameter>hdfs</parameter> destination, include the path to the directory where you copied the required libraries (see <xref linkend="destination-hdfs-prerequisites"/>), for example, <userinput>client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/*.jar:/opt/hadoop/libs/*.jar")</userinput>.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-diskbuffer.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-frac-digits.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="option-hdfs-append-enabled">
            <xi:include href="../../shared/chunk/option-destination-hdfs-append-enabled.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-archive-dir">
            <title>hdfs-archive-dir()</title>
            <indexterm type="parameter">
                <primary>hdfs-archive-dir()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs-archive-dir</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path where &abbrev; will move the closed log files. If &abbrev; cannot move the file for some reason (for example, &abbrev; cannot connect to the HDFS NameNode), the file remains at its original location. For example, <userinput>hdfs-archive-dir("/usr/hdfs/archive/")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-file">
            <title>hdfs-file()</title>
            <indexterm type="parameter">
                <primary>hdfs-file()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs-file</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path and name of the log file. For example, <userinput>hdfs-file("/usr/hdfs/mylogfile.txt")</userinput>. &abbrev; checks if the path to the logfile exists. If a directory does not exist &abbrev; automatically creates it.</para>
            <para><parameter>hdfs-file()</parameter> supports the usage of macros. This means that &abbrev; can create files on HDFS dynamically, using macros in the file (or directory) name.</para>
            <note>
                <para>When a filename resolved from the macros contains a character that HDFS does not support, &abbrev; will not be able to create the file. Make sure that you use macros that do not contain unsupported characters.</para>
            </note>
            <example>
                <title>Using macros in filenames</title>
                <para>In the following example, a <filename>/var/testdb_working_dir/$DAY-$HOUR.txt</filename> file will be created (with a UUID suffix):</para>
                <synopsis>destination d_hdfs_9bf3ff45341643c69bf46bfff940372a {
    hdfs(client-lib-dir(/hdfs-libs)
 hdfs-uri("hdfs://hdp2.syslog-ng.balabit:8020")
 hdfs-file("/var/testdb_working_dir/$DAY-$HOUR.txt"));
};</synopsis>
                <para>As an example, if it is the 31st day of the month and it is 12 o'clock, then the name of the file will be <filename>31-12.txt</filename>.</para>
            </example>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-max-filename-length">
            <title>hdfs-max-filename-length()</title>
            <indexterm type="parameter">
                <primary>hdfs-max-filename-length()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs-max-filename-length</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>number</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>255</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The maximum length of the filename. This filename (including the UUID that &abbrev; appends to it) cannot be longer than what the file system permits. If the filename is longer than the value of <parameter>hdfs-max-filename-length</parameter>, &abbrev; will automatically truncate the filename. For example, <userinput>hdfs-max-filename-length("255")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-resources">
            <title>hdfs-resources()</title>
            <indexterm type="parameter">
                <primary>hdfs-resources()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs-resources</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The list of Hadoop resources to load, separated by semicolons. For example, <userinput>hdfs-resources("/home/user/hadoop/core-site.xml;/home/user/hadoop/hdfs-site.xml")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-uri">
            <title>hdfs-uri()</title>
            <indexterm type="parameter">
                <primary>hdfs-uri()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs-uri</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The URI of the HDFS NameNode is in <userinput>hdfs://IPaddress:port</userinput> or <userinput>hdfs://hostname:port</userinput> format. When using MapR-FS, the URI of the MapR-FS NameNode is in <userinput>maprfs://IPaddress</userinput> or <userinput>maprfs://hostname</userinput> format, for example: <userinput>maprfs://10.140.32.80</userinput>. The IP address of the node can be IPv4 or IPv6. For example, <userinput>hdfs-uri("hdfs://10.140.32.80:8020")</userinput>. The IPv6 address must be enclosed in square brackets (<emphasis>[]</emphasis>) as specified by RFC 2732, for example, <userinput>hdfs-uri("hdfs://[FEDC:BA98:7654:3210:FEDC:BA98:7654:3210]:8020")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-destination-jvm-options">
            <xi:include href="../../shared/chunk/option-destination-jvm-options.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-kerberos-keytab-file">
            <title>kerberos-keytab-file()</title>
            <indexterm type="parameter">
                <primary>kerberos-keytab-file()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>kerberos-keytab-file</secondary>
            </indexterm>
            <indexterm>
                <primary>kerberos</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path to the Kerberos keytab file that you received from your Kerberos administrator. For example, <userinput>kerberos-keytab-file("/opt/syslog-ng/etc/hdfs.headless.keytab")</userinput>. This option is needed only if you want to authenticate using Kerberos in Hadoop. You also have to set the <link linkend="hdfs-option-kerberos-principal"><parameter>hdfs-option-kerberos-principal()</parameter></link> option. For details on the using Kerberos authentication with the <parameter>hdfs()</parameter> destination, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
            <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>Available in &abbrev; version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-kerberos-principal">
            <title>kerberos-principal()</title>
            <indexterm type="parameter">
                <primary>kerberos-principal()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>kerberos-principal</secondary>
            </indexterm>
            <indexterm>
                <primary>kerberos</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:</entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:</entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The Kerberos principal you want to authenticate with. For example, <userinput>kerberos-principal("hdfs-user@MYREALM")</userinput>. This option is needed only if you want to authenticate using Kerberos in Hadoop. You also have to set the <link linkend="hdfs-option-kerberos-keytab-file"><parameter>hdfs-option-kerberos-keytab-file()</parameter></link> option. For details on the using Kerberos authentication with the <parameter>hdfs()</parameter> destination, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
            <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>Available in &abbrev; version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-log-fifo-size.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-on-error.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-retries.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-template.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-throttle.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-timezone.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-ts-format.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
    </section>
</section>
